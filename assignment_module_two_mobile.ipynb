{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41535fd",
   "metadata": {},
   "source": [
    "# Assignment Module 2: Pet Classification\n",
    "\n",
    "The goal of this assignment is to implement a neural network that classifies images of 37 breeds of cats and dogs from the [Oxford-IIIT-Pet dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/). The assignment is divided into two parts: first, you will be asked to implement from scratch your own neural network for image classification; then, you will fine-tune a pretrained network provided by PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1476550",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The following cells contain the code to download and access the dataset you will be using in this assignment. Note that, although this dataset features each and every image from [Oxford-IIIT-Pet](https://www.robots.ox.ac.uk/~vgg/data/pets/), it uses a different train-val-test split than the original authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91101a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ipcv-assignment-2' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/CVLAB-Unibo/ipcv-assignment-2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d8fb0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ea1e31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA availability\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "fix_random(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b99c9929",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OxfordPetDataset(Dataset):\n",
    "    def __init__(self, split: str, transform=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.root = Path(\"ipcv-assignment-2\") / \"dataset\"\n",
    "        self.split = split\n",
    "        self.names, self.labels = self._get_names_and_labels()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n",
    "        img_path = self.root / \"images\" / f\"{self.names[idx]}.jpg\"\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "    def get_num_classes(self) -> int:\n",
    "        return max(self.labels) + 1\n",
    "\n",
    "    def _get_names_and_labels(self) -> Tuple[List[str], List[int]]:\n",
    "        names = []\n",
    "        labels = []\n",
    "\n",
    "        with open(self.root / \"annotations\" / f\"{self.split}.txt\") as f:\n",
    "            for line in f:\n",
    "                name, label = line.replace(\"\\n\", \"\").split(\" \")\n",
    "                names.append(name), \n",
    "                labels.append(int(label) - 1)\n",
    "\n",
    "        return names, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e655bd",
   "metadata": {},
   "source": [
    "## Part 1: design your own network\n",
    "\n",
    "Your goal is to implement a convolutional neural network for image classification and train it from scratch on `OxfordPetDataset`. You should consider yourselves satisfied once you obtain a classification accuracy on the test split of ~60%. You are free to achieve this however you want, except for a few rules you must follow:\n",
    "\n",
    "- Compile this notebook by displaying the results obtained by the best model you found throughout your experimentation; then show how, by removing some of its components, its performance drops. In other words, do an *ablation study* to prove that your design choices have a positive impact on the final result.\n",
    "\n",
    "- Do not instantiate an off-the-self PyTorch network. Instead, construct your network as a composition of existing PyTorch layers. In more concrete terms, you can use e.g. `torch.nn.Linear`, but you cannot use e.g. `torchvision.models.alexnet`.\n",
    "\n",
    "- Show your results and ablations with plots, tables, images, etc. — the clearer, the better.\n",
    "\n",
    "Don't be too concerned with your model performance: the ~60% is just to give you an idea of when to stop. Keep in mind that a thoroughly justified model with lower accuracy will be rewarded more points than a poorly experimentally validated model with higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a25ddb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 37\n",
      "Training samples: 3669\n",
      "Validation samples: 1834\n",
      "Test samples: 1846\n"
     ]
    }
   ],
   "source": [
    "# ImageNet mean and std for normalization\n",
    "IMG_SIZE = (224, 224) # A common size for image classification tasks\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.TrivialAugmentWide(),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD)\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD)\n",
    "])\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = OxfordPetDataset(split=\"train\", transform=train_transform)\n",
    "val_dataset = OxfordPetDataset(split=\"val\", transform=val_test_transform)\n",
    "test_dataset = OxfordPetDataset(split=\"test\", transform=val_test_transform)\n",
    "\n",
    "# Create DataLoader instances\n",
    "BATCH_SIZE = 128 # You can tune this hyperparameter\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# Get number of classes\n",
    "NUM_CLASSES = train_dataset.get_num_classes()\n",
    "INPUT_DIM = len(train_dataset[0][0])\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dda25217",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HSwish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * F.relu6(x + 3) / 6\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Conv2d(channels, channels // 4, 1)\n",
    "        self.fc2 = nn.Conv2d(channels // 4, channels, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        s = F.adaptive_avg_pool2d(x, 1)\n",
    "        s = F.relu(self.fc1(s))\n",
    "        s = F.relu6(self.fc2(s) + 3) / 6  # Hard sigmoid\n",
    "        return x * s\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, exp, out, k, s, se, hs):\n",
    "        super().__init__()\n",
    "        self.use_res = (s == 1 and inp == out)\n",
    "        \n",
    "        layers = []\n",
    "        # Expand\n",
    "        if exp != inp:\n",
    "            layers += [nn.Conv2d(inp, exp, 1, bias=False), nn.BatchNorm2d(exp), \n",
    "                      HSwish() if hs else nn.ReLU()]\n",
    "        # Depthwise\n",
    "        layers += [nn.Conv2d(exp, exp, k, s, k//2, groups=exp, bias=False), \n",
    "                  nn.BatchNorm2d(exp), HSwish() if hs else nn.ReLU()]\n",
    "        # SE\n",
    "        if se:\n",
    "            layers.append(SEBlock(exp))\n",
    "        # Project\n",
    "        layers += [nn.Conv2d(exp, out, 1, bias=False), nn.BatchNorm2d(out)]\n",
    "        \n",
    "        self.conv = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.use_res:\n",
    "            return x + self.conv(x)\n",
    "        return self.conv(x)\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "    def __init__(self, num_classes=37, mode='large'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # [inp, exp, out, kernel, stride, SE, HS]\n",
    "        if mode == 'large':\n",
    "            cfg = [\n",
    "                [16, 16, 16, 3, 1, 0, 0], [16, 64, 24, 3, 2, 0, 0], [24, 72, 24, 3, 1, 0, 0],\n",
    "                [24, 72, 40, 5, 2, 1, 0], [40, 120, 40, 5, 1, 1, 0], [40, 120, 40, 5, 1, 1, 0],\n",
    "                [40, 240, 80, 3, 2, 0, 1], [80, 200, 80, 3, 1, 0, 1], [80, 184, 80, 3, 1, 0, 1],\n",
    "                [80, 184, 80, 3, 1, 0, 1], [80, 480, 112, 3, 1, 1, 1], [112, 672, 112, 3, 1, 1, 1],\n",
    "                [112, 672, 160, 5, 2, 1, 1], [160, 960, 160, 5, 1, 1, 1], [160, 960, 160, 5, 1, 1, 1]\n",
    "            ]\n",
    "            last_ch = 960\n",
    "        else:  # small\n",
    "            cfg = [\n",
    "                [16, 16, 16, 3, 2, 1, 0], [16, 72, 24, 3, 2, 0, 0], [24, 88, 24, 3, 1, 0, 0],\n",
    "                [24, 96, 40, 5, 2, 1, 1], [40, 240, 40, 5, 1, 1, 1], [40, 240, 40, 5, 1, 1, 1],\n",
    "                [40, 120, 48, 5, 1, 1, 1], [48, 144, 48, 5, 1, 1, 1], [48, 288, 96, 5, 2, 1, 1],\n",
    "                [96, 576, 96, 5, 1, 1, 1], [96, 576, 96, 5, 1, 1, 1]\n",
    "            ]\n",
    "            last_ch = 576\n",
    "        \n",
    "        # Stem\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            HSwish()\n",
    "        )\n",
    "        \n",
    "        # Blocks\n",
    "        layers = []\n",
    "        for inp, exp, out, k, s, se, hs in cfg:\n",
    "            layers.append(InvertedResidual(inp, exp, out, k, s, se, hs))\n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "        \n",
    "        # Head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(cfg[-1][2], last_ch, 1, bias=False),\n",
    "            nn.BatchNorm2d(last_ch),\n",
    "            HSwish(),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(last_ch, 1280 if mode == 'large' else 1024),\n",
    "            HSwish(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1280 if mode == 'large' else 1024, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.head(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def mobilenetv3_large(num_classes=37):\n",
    "    return MobileNetV3(num_classes, 'large')\n",
    "\n",
    "def mobilenetv3_small(num_classes=37):\n",
    "    return MobileNetV3(num_classes, 'small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e4a6876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model: nn.Module,\n",
    "                dataloader: DataLoader,\n",
    "                criterion: nn.Module,\n",
    "                optimizer: optim.Optimizer,\n",
    "                device: torch.device,\n",
    "                scheduler: Optional[lr_scheduler.LRScheduler] = None) -> Tuple[float, float]:\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None: \n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = epoch_loss / total_samples\n",
    "    avg_acc = correct_predictions.double() / total_samples\n",
    "    return avg_loss, avg_acc.item()\n",
    "\n",
    "def evaluate_model(model: nn.Module,\n",
    "                   dataloader: DataLoader,\n",
    "                   criterion: nn.Module,\n",
    "                   device: torch.device) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(preds == labels.data)\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = epoch_loss / total_samples\n",
    "    avg_acc = correct_predictions.double() / total_samples\n",
    "    return avg_loss, avg_acc.item()\n",
    "\n",
    "def plot_history(history: Dict[str, List[float]]):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32ded480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([1, 37])\n",
      "Params: 4,247,595\n"
     ]
    }
   ],
   "source": [
    "model = mobilenetv3_large(num_classes=37).to(DEVICE)\n",
    "x = torch.randn(1, 3, 224, 224).to(DEVICE)\n",
    "print(f\"Output: {model(x).shape}\")\n",
    "print(f\"Params: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94c250bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Training: 100%|█| 50/50 [2:30:36<00:00, 180.73s/it, Train Loss=1.6871, Val Loss=1.5185, Train Acc=0.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "# Then train with your existing functions:\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "best_val_acc = 0.0\n",
    "\n",
    "# main progress bar over epochs\n",
    "pbar = tqdm(range(EPOCHS), desc=f\"Training\", ncols=100)\n",
    "\n",
    "for epoch in pbar:\n",
    "    # --- Training ---\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    \n",
    "    # --- Validation ---\n",
    "    val_loss, val_acc = evaluate_model(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    # --- Record metrics ---\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    pbar.set_postfix({\n",
    "        \"Train Loss\": f\"{train_loss:.4f}\",\n",
    "        \"Val Loss\": f\"{val_loss:.4f}\",\n",
    "        \"Train Acc\": f\"{train_acc:.4f}\",\n",
    "        \"Val Acc\": f\"{val_acc:.4f}\"    \n",
    "        })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipcvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
