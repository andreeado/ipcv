{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d41535fd",
      "metadata": {
        "id": "d41535fd"
      },
      "source": [
        "# Assignment Module 2: Pet Classification\n",
        "\n",
        "The goal of this assignment is to implement a neural network that classifies images of 37 breeds of cats and dogs from the [Oxford-IIIT-Pet dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/). The assignment is divided into two parts: first, you will be asked to implement from scratch your own neural network for image classification; then, you will fine-tune a pretrained network provided by PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1476550",
      "metadata": {
        "id": "b1476550"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The following cells contain the code to download and access the dataset you will be using in this assignment. Note that, although this dataset features each and every image from [Oxford-IIIT-Pet](https://www.robots.ox.ac.uk/~vgg/data/pets/), it uses a different train-val-test split than the original authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d8fb0d2",
      "metadata": {
        "id": "0d8fb0d2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcbc7d65",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_paths = [\"/Users/leonardomassaro/Desktop/DatasetOxford_prof/\", \"/scratch.hpc/leomass/ipcv-assignment-2/dataset/\", \"/scratch.hpc/leonardo.massaro2/ipcv-assignment-2/dataset/\"]\n",
        "correct_path = None\n",
        "\n",
        "for path in dataset_paths:\n",
        "    if os.path.exists(path) and os.path.isdir(path):\n",
        "        print(\"Detected dataset on \", path)\n",
        "        correct_path = path\n",
        "if not correct_path:\n",
        "    raise Exception(\"No dataset found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea1e31f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ea1e31f",
        "outputId": "5e125b49-0307-4144-da36-4121a3e3af7a"
      },
      "outputs": [],
      "source": [
        "# Check for CUDA availability\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "def fix_random(seed: int) -> None:\n",
        "    \"\"\"Fix all the possible sources of randomness.\n",
        "\n",
        "    Args:\n",
        "        seed: the seed to use.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "fix_random(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b99c9929",
      "metadata": {
        "id": "b99c9929"
      },
      "outputs": [],
      "source": [
        "class OxfordPetDataset(Dataset):\n",
        "    def __init__(self, correct_path,  split: str, transform=None) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.root = Path(correct_path)\n",
        "        self.split = split\n",
        "        self.names, self.labels = self._get_names_and_labels()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n",
        "        img_path = self.root / \"images\" / f\"{self.names[idx]}.jpg\"\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def get_num_classes(self) -> int:\n",
        "        return max(self.labels) + 1\n",
        "\n",
        "    def _get_names_and_labels(self) -> Tuple[List[str], List[int]]:\n",
        "        names = []\n",
        "        labels = []\n",
        "\n",
        "        with open(self.root / \"annotations\" / f\"{self.split}.txt\") as f:\n",
        "            for line in f:\n",
        "                name, label = line.replace(\"\\n\", \"\").split(\" \")\n",
        "                names.append(name),\n",
        "                labels.append(int(label) - 1)\n",
        "\n",
        "        return names, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4e655bd",
      "metadata": {
        "id": "b4e655bd"
      },
      "source": [
        "## Part 1: design your own network\n",
        "\n",
        "Your goal is to implement a convolutional neural network for image classification and train it from scratch on `OxfordPetDataset`. You should consider yourselves satisfied once you obtain a classification accuracy on the test split of ~60%. You are free to achieve this however you want, except for a few rules you must follow:\n",
        "\n",
        "- Compile this notebook by displaying the results obtained by the best model you found throughout your experimentation; then show how, by removing some of its components, its performance drops. In other words, do an *ablation study* to prove that your design choices have a positive impact on the final result.\n",
        "\n",
        "- Do not instantiate an off-the-self PyTorch network. Instead, construct your network as a composition of existing PyTorch layers. In more concrete terms, you can use e.g. `torch.nn.Linear`, but you cannot use e.g. `torchvision.models.alexnet`.\n",
        "\n",
        "- Show your results and ablations with plots, tables, images, etc. — the clearer, the better.\n",
        "\n",
        "Don't be too concerned with your model performance: the ~60% is just to give you an idea of when to stop. Keep in mind that a thoroughly justified model with lower accuracy will be rewarded more points than a poorly experimentally validated model with higher accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a25ddb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a25ddb4",
        "outputId": "263ba953-245d-457f-91a8-57866f8c1779"
      },
      "outputs": [],
      "source": [
        "# ImageNet mean and std for normalization\n",
        "IMG_SIZE = (224, 224) # A common size for image classification tasks\n",
        "MEAN = [0.485, 0.456, 0.406]\n",
        "STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.TrivialAugmentWide(),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD)\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD)\n",
        "])\n",
        "\n",
        "# Create Dataset instances\n",
        "train_dataset = OxfordPetDataset(correct_path=correct_path,split=\"train\", transform=train_transform)\n",
        "val_dataset = OxfordPetDataset(correct_path=correct_path,split=\"val\", transform=val_test_transform)\n",
        "test_dataset = OxfordPetDataset(correct_path=correct_path,split=\"test\", transform=val_test_transform)\n",
        "\n",
        "# Create DataLoader instances\n",
        "BATCH_SIZE = 128 # You can tune this hyperparameter\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Get number of classes\n",
        "NUM_CLASSES = train_dataset.get_num_classes()\n",
        "INPUT_DIM = len(train_dataset[0][0])\n",
        "print(f\"Number of classes: {NUM_CLASSES}\")\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "print(train_dataset[0][0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f2a6c25",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Defining custom Dense Block\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, num_convs:int, growth_rate:int, has_transition:bool):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        self.num_convs = num_convs\n",
        "        self.growth_rate = growth_rate\n",
        "        layer = []\n",
        "        for i in range(num_convs):\n",
        "            layer.append(self.conv_block(growth_rate))\n",
        "        self.dense_net = nn.Sequential(*layer)\n",
        "        self.transition_net = self.transition_block() if has_transition else None\n",
        "\n",
        "    def conv_block(self, num_channels):\n",
        "        return (\n",
        "            nn.Sequential(\n",
        "                nn.LazyBatchNorm2d(),\n",
        "                nn.ReLU(),\n",
        "                nn.LazyConv2d(num_channels, 3, padding=1))\n",
        "        )\n",
        "    \n",
        "    def transition_block(self): #transition block to reduce channel dimensionality (conv has to be 1x1)\n",
        "        compression_factor = 0.5\n",
        "        num_transition_channels = int(compression_factor * (self.growth_rate * self.num_convs))\n",
        "        return (\n",
        "            nn.Sequential(\n",
        "                nn.LazyBatchNorm2d(),\n",
        "                nn.ReLU(),\n",
        "                nn.LazyConv2d(num_transition_channels, 1),\n",
        "                nn.AvgPool2d(2, stride=2)\n",
        "                )\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        # In the Dense block, every layer has in input the concatenation of all the previous ones\n",
        "        for block in self.dense_net:\n",
        "            last_out = block(X)\n",
        "            X = torch.cat((X, last_out), dim=1)\n",
        "        if self.transition_net != None:\n",
        "            X = self.transition_net(X)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f18e22ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, num_dense_blocks, num_conv_each_dense, growth_rate):\n",
        "        super().__init__()\n",
        "        stem_block = [ #initial stem block, like resnet\n",
        "            nn.LazyConv2d(32, 7, stride=2),\n",
        "            nn.LazyBatchNorm2d(),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "        ]\n",
        "\n",
        "        dense_core = [\n",
        "            DenseBlock(num_convs=num_conv_each_dense[i], growth_rate=growth_rate, has_transition= True ) for i in range(num_dense_blocks)\n",
        "        ]\n",
        "\n",
        "        classification_block = [\n",
        "            nn.LazyBatchNorm2d(),\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.LazyLinear(37)\n",
        "        ]\n",
        "\n",
        "        self.net_stack = nn.Sequential(*stem_block, *dense_core, *classification_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        result = self.net_stack(x)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e4a6876",
      "metadata": {
        "id": "1e4a6876"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model: nn.Module,\n",
        "                dataloader: DataLoader,\n",
        "                criterion: nn.Module,\n",
        "                optimizer: optim.Optimizer,\n",
        "                device: torch.device,\n",
        "                scheduler: Optional[lr_scheduler.LRScheduler] = None) -> Tuple[float, float]:\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        epoch_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct_predictions += torch.sum(preds == labels.data)\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    avg_loss = epoch_loss / total_samples\n",
        "    avg_acc = correct_predictions.double() / total_samples\n",
        "    return avg_loss, avg_acc.item()\n",
        "\n",
        "def evaluate_model(model: nn.Module,\n",
        "                   dataloader: DataLoader,\n",
        "                   criterion: nn.Module,\n",
        "                   device: torch.device) -> Tuple[float, float]:\n",
        "    model.eval()\n",
        "    epoch_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            epoch_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct_predictions += torch.sum(preds == labels.data)\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    avg_loss = epoch_loss / total_samples\n",
        "    avg_acc = correct_predictions.double() / total_samples\n",
        "    return avg_loss, avg_acc.item()\n",
        "\n",
        "def plot_history(history: Dict[str, List[float]]):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Over Epochs')\n",
        "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
        "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy Over Epochs')\n",
        "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94c250bd",
      "metadata": {
        "id": "94c250bd"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 120\n",
        "\n",
        "def run_experiment(config: Dict, model = None):\n",
        "    print(\"=\"*60 + f\"\\nStart running the model: {config['experiment_name']}\\n\" + \"=\"*60)\n",
        "    fix_random(42)\n",
        "\n",
        "    model = config[\"model\"]\n",
        "\n",
        "    # Setup of Early stopping, LR schedule and weight decay\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.get(\"lr\", 1e-3), weight_decay=config['weight_decay'])\n",
        "    if config['use_scheduler']:\n",
        "        warmup_scheduler = lr_scheduler.LinearLR(optimizer, start_factor=0.01, end_factor=1.0, total_iters=5)\n",
        "        main_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=15)\n",
        "\n",
        "    # Training Loop\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "    best_val_acc = 0.0\n",
        "    pbar = tqdm(range(EPOCHS), desc=f\"Training {config['experiment_name']}\")\n",
        "    for epoch in pbar:\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, DEVICE)\n",
        "        history['train_loss'].append(train_loss); history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n",
        "        if config['use_scheduler']:\n",
        "            if epoch < 5: warmup_scheduler.step()\n",
        "            else: main_scheduler.step(val_acc)\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), f\"{config['experiment_name']}.pt\")\n",
        "            #print(f\"Saved new best model, val acc: {val_acc}\")\n",
        "        pbar.set_postfix({\"Val Acc\": f\"{val_acc:.4f}\", \"Train Acc\": f\"{train_acc:.4f}\"})\n",
        "\n",
        "    # Final test and save result\n",
        "    sd = torch.load(f\"{config['experiment_name']}.pt\")\n",
        "    model.load_state_dict(sd)\n",
        "    _, test_acc = evaluate_model(model, test_loader, criterion, DEVICE)\n",
        "    print(f\"Model '{config['experiment_name']}' completed. Test Accuracy: {test_acc:.4f}\\n\")\n",
        "\n",
        "    result = config.copy()\n",
        "    result.update({'test_accuracy': test_acc, 'best_val_accuracy': best_val_acc, 'history': history})\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c438dc70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "c438dc70",
        "outputId": "ded4b823-2973-4344-90d0-5d5bf15df1aa"
      },
      "outputs": [],
      "source": [
        "mobile_configs = [{\n",
        "    'experiment_name': 'densenet_small',\n",
        "    'lr': 1e-3,\n",
        "    'weight_decay': 1e-4,\n",
        "    'use_scheduler': True,\n",
        "    \"model\": NeuralNetwork(num_dense_blocks=3, num_conv_each_dense=[6,16,18], growth_rate=12).to(DEVICE)\n",
        "}\n",
        "]\n",
        "\n",
        "for conf in mobile_configs:\n",
        "    result= run_experiment(conf)\n",
        "    df = pd.DataFrame(result).to_csv(f\"./output/{result['experiment_name']}.csv\")\n",
        "    plot_history(result['history'])\n",
        "    print(f\"\\nFinal Results for {result['experiment_name']}:\")\n",
        "    print(f\"Best Validation Accuracy: {result['best_val_accuracy']:.4f}\")\n",
        "    print(f\"Test Accuracy: {result['test_accuracy']:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
