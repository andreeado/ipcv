{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b6ac8eac",
      "metadata": {
        "id": "b6ac8eac"
      },
      "source": [
        "# Assignment Module 2: Pet Classification\n",
        "\n",
        "The goal of this assignment is to implement a neural network that classifies images of 37 breeds of cats and dogs from the [Oxford-IIIT-Pet dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/). The assignment is divided into two parts: first, you will be asked to implement from scratch your own neural network for image classification; then, you will fine-tune a pretrained network provided by PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "37b34106",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37b34106",
        "outputId": "dd25fa02-d1bf-409a-a1b6-4424244de9c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ipcv-assignment-2'...\n",
            "remote: Enumerating objects: 7371, done.\u001b[K\n",
            "remote: Total 7371 (delta 0), reused 0 (delta 0), pack-reused 7371 (from 1)\u001b[K\n",
            "Receiving objects: 100% (7371/7371), 753.77 MiB | 32.78 MiB/s, done.\n",
            "Updating files: 100% (7396/7396), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/CVLAB-Unibo/ipcv-assignment-2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fb61443f",
      "metadata": {
        "id": "fb61443f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import numpy as np\n",
        "import random\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision.transforms import v2 as transforms_v2\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.io import decode_image\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import time\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "from torchvision.models import resnet18, ResNet18_Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d4cf8449",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4cf8449",
        "outputId": "2021a4bc-b652-4d11-b526-d11783b9ab4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6dfbafe0",
      "metadata": {
        "id": "6dfbafe0"
      },
      "outputs": [],
      "source": [
        "def fix_random(seed: int) -> None:\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "fix_random(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4e332c19",
      "metadata": {
        "id": "4e332c19"
      },
      "outputs": [],
      "source": [
        "class OxfordPetDataset(Dataset):\n",
        "    def __init__(self, split: str, transform=None) -> None:\n",
        "        super().__init__()\n",
        "        self.root = Path(\"ipcv-assignment-2\") / \"dataset\"\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.names, self.labels = self._get_names_and_labels()\n",
        "        tot_size_bytes = 0\n",
        "        n_images = 0\n",
        "        loaded_data = []\n",
        "        idx = 0\n",
        "        while idx < len(self.names):\n",
        "            img_tensor = self.get_img_from_filesystem(idx)\n",
        "            tot_size_bytes += img_tensor.numel() * img_tensor.element_size()\n",
        "            n_images += 1\n",
        "            loaded_data.append(img_tensor)\n",
        "            idx += 1\n",
        "        print(\"tot size\", tot_size_bytes, \"bytes, for\", n_images, \"images\")\n",
        "        self.data_tensor = torch.stack(loaded_data).to(device=device)\n",
        "        self.labels = torch.Tensor(self.labels).type(torch.LongTensor).to(device=device)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        img = self.data_tensor[idx]\n",
        "        img = self.transform(img).to(device)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def get_img_from_filesystem(self, idx) -> Tensor:\n",
        "        img_path = self.root / \"images\" / f\"{self.names[idx]}.jpg\"\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = transforms_v2.Resize((256,256))(img)\n",
        "        img = transforms_v2.ToTensor()(img)\n",
        "        return img\n",
        "\n",
        "\n",
        "    def get_num_classes(self) -> int:\n",
        "        return max(self.labels) + 1\n",
        "\n",
        "    def _get_names_and_labels(self) -> Tuple[List[str], List[int]]:\n",
        "        names = []\n",
        "        labels = []\n",
        "\n",
        "        with open(self.root / \"annotations\" / f\"{self.split}.txt\") as f:\n",
        "            for line in f:\n",
        "                if(line[0] == \"#\"):\n",
        "                    continue\n",
        "                name, label = line.replace(\"\\n\", \"\").split(\" \")\n",
        "                names.append(name),\n",
        "                labels.append(int(label) - 1)\n",
        "\n",
        "        return names, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "569547f6",
      "metadata": {
        "id": "569547f6"
      },
      "outputs": [],
      "source": [
        "def get_model_resnet18():\n",
        "    # Define the model and local path for saving weights\n",
        "    model_resnet_18 = models.resnet18()\n",
        "    weights_path = \"./resnet18_weights.pth\"\n",
        "\n",
        "    # Check if the file exists locally\n",
        "    if os.path.exists(weights_path):\n",
        "        print(\"âœ… Loading existing ResNet-18 weights...\")\n",
        "        state_dict = torch.load(weights_path, map_location=\"cpu\")\n",
        "        model_resnet_18.load_state_dict(state_dict)\n",
        "    else:\n",
        "        print(\"â¬‡ï¸  Downloading ResNet-18 weights...\")\n",
        "        model_resnet_18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)  # Downloads pretrained weights\n",
        "        torch.save(model_resnet_18.state_dict(), weights_path)\n",
        "        print(\"ðŸ’¾ Weights saved locally at:\", weights_path)\n",
        "    return model_resnet_18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "62ed9a5e",
      "metadata": {
        "id": "62ed9a5e"
      },
      "outputs": [],
      "source": [
        "MEAN = [0.485, 0.456, 0.406]\n",
        "STD = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99529acc",
      "metadata": {
        "id": "99529acc"
      },
      "source": [
        "## Part 1: design your own network\n",
        "\n",
        "Your goal is to implement a convolutional neural network for image classification and train it from scratch on `OxfordPetDataset`. You should consider yourselves satisfied once you obtain a classification accuracy on the test split of ~60%. You are free to achieve this however you want, except for a few rules you must follow:\n",
        "\n",
        "- Compile this notebook by displaying the results obtained by the best model you found throughout your experimentation; then show how, by removing some of its components, its performance drops. In other words, do an *ablation study* to prove that your design choices have a positive impact on the final result.\n",
        "\n",
        "- Do not instantiate an off-the-self PyTorch network. Instead, construct your network as a composition of existing PyTorch layers. In more concrete terms, you can use e.g. `torch.nn.Linear`, but you cannot use e.g. `torchvision.models.alexnet`.\n",
        "\n",
        "- Show your results and ablations with plots, tables, images, etc. â€” the clearer, the better.\n",
        "\n",
        "Don't be too concerned with your model performance: the ~60% is just to give you an idea of when to stop. Keep in mind that a thoroughly justified model with lower accuracy will be rewarded more points than a poorly experimentally validated model with higher accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a78db29d",
      "metadata": {
        "id": "a78db29d"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Â Defining custom Dense Block\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, num_convs:int, growth_rate:int, has_transition:bool = True):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        self.num_convs = num_convs\n",
        "        self.growth_rate = growth_rate\n",
        "        layer = []\n",
        "        for i in range(num_convs):\n",
        "            layer.append(self.conv_block(growth_rate))\n",
        "        self.dense_net = nn.Sequential(*layer)\n",
        "        self.transition_net = self.transition_block() if has_transition else None\n",
        "\n",
        "    def conv_block(self, num_channels):\n",
        "        return (\n",
        "            nn.Sequential(\n",
        "                nn.LazyBatchNorm2d(),\n",
        "                nn.ReLU(),\n",
        "                nn.LazyConv2d(num_channels, 3, padding=1))\n",
        "        )\n",
        "\n",
        "    def transition_block(self): #transition block to reduce channel dimensionality (conv has to be 1x1)\n",
        "        compression_factor = 0.5\n",
        "        num_transition_channels = int(compression_factor * (self.growth_rate * self.num_convs))\n",
        "        return (\n",
        "            nn.Sequential(\n",
        "                nn.LazyBatchNorm2d(),\n",
        "                nn.ReLU(),\n",
        "                nn.LazyConv2d(num_transition_channels, 1),\n",
        "                nn.AvgPool2d(2, stride=2)\n",
        "                )\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        # In the Dense block, every layer has in input the concatenation of all the previous ones\n",
        "        for block in self.dense_net:\n",
        "            last_out = block(X)\n",
        "            X = torch.cat((X, last_out), dim=1)\n",
        "        if self.transition_net != None:\n",
        "            X = self.transition_net(X)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0cf76ccd",
      "metadata": {
        "id": "0cf76ccd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, num_dense_blocks, num_conv_each_dense, growth_rate, dropout):\n",
        "        super().__init__()\n",
        "        stem_block = [ #initial stem block, like resnet\n",
        "            nn.LazyConv2d(32, 7, stride=2),\n",
        "            nn.LazyBatchNorm2d(),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "        ]\n",
        "\n",
        "        dense_core = [\n",
        "            DenseBlock(num_convs=num_conv_each_dense[i], growth_rate=growth_rate, has_transition= True ) for i in range(num_dense_blocks)\n",
        "        ]\n",
        "\n",
        "        classification_block = [\n",
        "            nn.LazyBatchNorm2d(),\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.LazyLinear(37)\n",
        "        ]\n",
        "\n",
        "        self.net_stack = nn.Sequential(*stem_block, *dense_core, *classification_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        result = self.net_stack(x)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "19d83924",
      "metadata": {
        "id": "19d83924"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer, scheduler, lr_data):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for X, y in dataloader:\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        lr_data.append(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a720e62c",
      "metadata": {
        "id": "a720e62c"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    accuracy = 100*correct\n",
        "    return accuracy, test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1abc4ac6",
      "metadata": {
        "id": "1abc4ac6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_model(test_loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs, labels\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            all_preds.append(preds)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    # --- Accuracy ---\n",
        "    accuracy = correct / total\n",
        "\n",
        "    # --- Confusion Matrix ---\n",
        "    num_classes = outputs.shape[1]\n",
        "    conf_matrix = torch.zeros(num_classes, num_classes, dtype=torch.int64)\n",
        "    for t, p in zip(all_labels, all_preds):\n",
        "        conf_matrix[t, p] += 1\n",
        "\n",
        "    # --- Precision, Recall, F1 ---\n",
        "    tp = conf_matrix.diag()\n",
        "    fp = conf_matrix.sum(dim=0) - tp\n",
        "    fn = conf_matrix.sum(dim=1) - tp\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-12)\n",
        "    recall = tp / (tp + fn + 1e-12)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-12)\n",
        "\n",
        "    # Weighted (by class support)\n",
        "    support = conf_matrix.sum(dim=1)\n",
        "    weighted_precision = (precision * support).sum() / support.sum()\n",
        "    weighted_recall = (recall * support).sum() / support.sum()\n",
        "    weighted_f1 = (f1 * support).sum() / support.sum()\n",
        "\n",
        "    return {\n",
        "        \"test_accuracy\": accuracy,\n",
        "        \"test_precision\": weighted_precision.item(),\n",
        "        \"test_recall\": weighted_recall.item(),\n",
        "        \"test_f1\": weighted_f1.item()\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac524c4d",
      "metadata": {
        "id": "ac524c4d"
      },
      "outputs": [],
      "source": [
        "## declaration of data augumentation stacks\n",
        "training_data_augumentation_stacks = {\n",
        "    \"default\" : transforms_v2.Compose([\n",
        "        transforms_v2.Resize((256,256)),\n",
        "        transforms_v2.RandomResizedCrop(size=(224, 224)),\n",
        "        transforms_v2.RandomHorizontalFlip(p=0.5),\n",
        "        transforms_v2.ColorJitter(\n",
        "            brightness=0.5,\n",
        "            contrast=0.5,\n",
        "            saturation=0.5,\n",
        "            hue=0.1\n",
        "        ),\n",
        "        transforms_v2.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 1.0)),\n",
        "        transforms_v2.ToDtype(torch.float32, scale=True),\n",
        "        transforms_v2.Normalize(mean=MEAN,\n",
        "                    std=STD),\n",
        "    ]),\n",
        "\n",
        "    \"same_as_default_but_lighter\" : transforms_v2.Compose([\n",
        "        transforms_v2.Resize((256,256)),\n",
        "        transforms_v2.RandomResizedCrop(size=(224, 224)),\n",
        "        transforms_v2.RandomHorizontalFlip(p=0.5),\n",
        "        transforms_v2.ColorJitter(\n",
        "            brightness=0.1,\n",
        "            contrast=0.1,\n",
        "            saturation=0.1,\n",
        "            hue=0.05\n",
        "        ),\n",
        "        transforms_v2.GaussianBlur(kernel_size=(3, 3), sigma=(0.2)),\n",
        "        transforms_v2.ToDtype(torch.float32, scale=True),\n",
        "        transforms_v2.Normalize(mean=MEAN,\n",
        "                    std=STD),\n",
        "    ]),\n",
        "\"trivial_augument_wide\": transforms_v2.Compose([\n",
        "    transforms_v2.Resize((256,256)),\n",
        "    transforms_v2.TrivialAugmentWide(),\n",
        "    transforms_v2.RandomResizedCrop(size=(224, 224)),\n",
        "    transforms_v2.ToDtype(torch.float32, scale=True),\n",
        "    transforms_v2.Normalize(MEAN, STD),\n",
        "])\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "93ca4d22",
      "metadata": {
        "id": "93ca4d22"
      },
      "outputs": [],
      "source": [
        "def run_experiment(parameters):\n",
        "    BATCH_SIZE = parameters.get(\"batch_size\", 64)\n",
        "    N_DENS_BLKS = parameters.get(\"n_dense_blocks\", 3)\n",
        "    N_CONVS_EACH_BLCK = parameters.get(\"n_conv_each_block\", [4,8,7])\n",
        "    GRWTH_RATE = parameters.get(\"growth_rate\", 12)\n",
        "    DROPOUT = parameters.get(\"dropout\", 0.1)\n",
        "    BASE_LR = parameters.get(\"base_learning_rate\", 0.001)\n",
        "    WARMUP_ITERS = parameters.get(\"warmup_iter\", 7500)\n",
        "    MAIN_SCH_KICK = parameters.get(\"main_sched_epochs_kick_in\", 60)\n",
        "    EPOCHS = parameters.get(\"n_epochs\", 100)\n",
        "    WEIGHT_DECAY = parameters.get(\"weight_decay\", 0.001)\n",
        "    TRAINING_TRANSFORM_STACK = training_data_augumentation_stacks[parameters.get(\"transform_stack_name\", \"default\")]\n",
        "    EXPERIMENT_TYPE = parameters.get(\"experiment_type\", \"default\")\n",
        "\n",
        "    validation_transform_stack = transforms_v2.Compose([\n",
        "        transforms_v2.Resize(256),\n",
        "        transforms_v2.CenterCrop(224),\n",
        "        transforms_v2.Normalize(MEAN, STD)\n",
        "    ])\n",
        "\n",
        "    train_dataset = OxfordPetDataset(split=\"train\" , transform=TRAINING_TRANSFORM_STACK)\n",
        "    validation_dataset = OxfordPetDataset(split=\"val\", transform=validation_transform_stack)\n",
        "    test_dataset = OxfordPetDataset(split=\"test\", transform=validation_transform_stack)\n",
        "\n",
        "    # Create data loaders.\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    if EXPERIMENT_TYPE == \"default\" or EXPERIMENT_TYPE == \"dense_net\":\n",
        "        model = NeuralNetwork(num_dense_blocks=N_DENS_BLKS, num_conv_each_dense=N_CONVS_EACH_BLCK, growth_rate=GRWTH_RATE, dropout=DROPOUT).to(device)\n",
        "    elif EXPERIMENT_TYPE == \"resnet18\":\n",
        "        model_resnet_18 = get_model_resnet18()\n",
        "        num_features = model_resnet_18.fc.in_features\n",
        "        model_resnet_18.fc = nn.Sequential(\n",
        "            nn.Dropout(DROPOUT),\n",
        "            nn.Linear(num_features, 37)\n",
        "        )\n",
        "        # Freeze everything\n",
        "        for param in model_resnet_18.parameters():\n",
        "            param.requires_grad = False\n",
        "        # Unfreeze only the head\n",
        "        for param in model_resnet_18.fc.parameters():\n",
        "            param.requires_grad = True\n",
        "        model = model_resnet_18.to(device)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    warmup_scheduler = lr_scheduler.LinearLR(optimizer, start_factor=0.01, end_factor=1., total_iters=WARMUP_ITERS)\n",
        "    main_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.01, patience=8)\n",
        "\n",
        "    batch_lrs = []           # record learning rate per batch\n",
        "    epoch_end_indices = []   # store index (batch count) at the end of each epoch\n",
        "\n",
        "    net_performance_data = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_accuracy\": []\n",
        "    }\n",
        "\n",
        "    t1 = time.time()\n",
        "\n",
        "    for t in range(EPOCHS):\n",
        "        net_performance_data[\"train_loss\"].append( train(train_dataloader, model, loss_fn, optimizer, warmup_scheduler, batch_lrs) )\n",
        "        val_accuracy, val_loss = test(validation_dataloader, model, loss_fn)\n",
        "        if t > MAIN_SCH_KICK: main_scheduler.step(val_accuracy)\n",
        "        net_performance_data[\"val_loss\"].append( val_loss )\n",
        "        net_performance_data[\"val_accuracy\"].append( val_accuracy )\n",
        "        epoch_end_indices.append(len(batch_lrs))\n",
        "\n",
        "    t2 = time.time()\n",
        "    elapsed_time = t2-t1\n",
        "\n",
        "    print(\"Done in \", elapsed_time, \" sec\")\n",
        "\n",
        "    print(\"Best accuracy \", np.max(net_performance_data[\"val_accuracy\"]))\n",
        "\n",
        "    net_performance_data_df = pd.DataFrame(data=net_performance_data)\n",
        "\n",
        "    #add all experiment parameters to the result dataframe\n",
        "    for key in parameters:\n",
        "        if isinstance(parameters[key], list):\n",
        "            for i, el in enumerate(parameters[key]):\n",
        "                net_performance_data_df[key + \"_\"+ str(i)] = el\n",
        "        else:\n",
        "            net_performance_data_df[key] = parameters[key]\n",
        "\n",
        "    # add elapsed time\n",
        "    net_performance_data_df[\"elapsed_time_sec\"] = elapsed_time\n",
        "\n",
        "    # create a column for epochs steps indexes\n",
        "    net_performance_data_df = net_performance_data_df.reset_index().rename(columns={\"index\":\"epoch\"})\n",
        "\n",
        "    # add final evaluation stats\n",
        "    eval_performaces = evaluate_model(test_dataloader, model)\n",
        "    for metric in eval_performaces:\n",
        "        net_performance_data_df[metric] = eval_performaces[metric]\n",
        "\n",
        "    return net_performance_data_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6ff85b1",
      "metadata": {
        "id": "b6ff85b1"
      },
      "outputs": [],
      "source": [
        "parameters_base_deactivated = [\n",
        "{\n",
        "    \"experiment_name\": \"24_grwth_rate_baseline\",\n",
        "    \"batch_size\" : 64,\n",
        "    \"n_dense_blocks\": 3,\n",
        "    \"n_conv_each_block\": [6,16,14],\n",
        "    \"growth_rate\": 24,\n",
        "    \"dropout\": 0.2,\n",
        "    \"base_learning_rate\": 0.001,\n",
        "    \"warmup_iter\": 7500,\n",
        "    \"main_sched_epochs_kick_in\": 70,\n",
        "    \"n_epochs\": 180,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"transform_stack_name\": \"default\"\n",
        "},\n",
        "{\n",
        "    \"experiment_name\": \"high_dropout\",\n",
        "    \"batch_size\" : 64,\n",
        "    \"n_dense_blocks\": 3,\n",
        "    \"n_conv_each_block\": [6,16,14],\n",
        "    \"growth_rate\": 24,\n",
        "    \"dropout\": 0.4,\n",
        "    \"base_learning_rate\": 0.001,\n",
        "    \"warmup_iter\": 7500,\n",
        "    \"main_sched_epochs_kick_in\": 70,\n",
        "    \"n_epochs\": 180,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"transform_stack_name\": \"default\"\n",
        "},\n",
        "{\n",
        "    \"experiment_name\": \"capacity_increase_blocks_growth\",\n",
        "    \"batch_size\": 64,\n",
        "    \"n_dense_blocks\": 4,\n",
        "    \"n_conv_each_block\": [6, 12, 16, 14],\n",
        "    \"growth_rate\": 32,\n",
        "    \"dropout\": 0.2,\n",
        "    \"base_learning_rate\": 0.001,\n",
        "    \"warmup_iter\": 7500,\n",
        "    \"main_sched_epochs_kick_in\": 70,\n",
        "    \"n_epochs\": 180,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"transform_stack_name\": \"default\"\n",
        "},\n",
        "{\n",
        "    \"experiment_name\": \"lightweight_model_ablation\",\n",
        "    \"batch_size\": 96,\n",
        "    \"n_dense_blocks\": 3,\n",
        "    \"n_conv_each_block\": [4, 10, 10],\n",
        "    \"growth_rate\": 16,\n",
        "    \"dropout\": 0.2,\n",
        "    \"base_learning_rate\": 0.001,\n",
        "    \"warmup_iter\": 5000,\n",
        "    \"main_sched_epochs_kick_in\": 60,\n",
        "    \"n_epochs\": 180,\n",
        "    \"weight_decay\": 0.0005,\n",
        "    \"transform_stack_name\": \"default\"\n",
        "},\n",
        "{\n",
        "    \"experiment_name\": \"deep_dense_blocks\",\n",
        "    \"batch_size\": 32,\n",
        "    \"n_dense_blocks\": 3,\n",
        "    \"n_conv_each_block\": [12, 24, 24],\n",
        "    \"growth_rate\": 24,\n",
        "    \"dropout\": 0.2,\n",
        "    \"base_learning_rate\": 0.0008,\n",
        "    \"warmup_iter\": 10000,\n",
        "    \"main_sched_epochs_kick_in\": 90,\n",
        "    \"n_epochs\": 180,\n",
        "    \"weight_decay\": 0.0015,\n",
        "    \"transform_stack_name\": \"default\"\n",
        "},\n",
        "{\n",
        "    \"experiment_name\": \"weight_decay_heavy\",\n",
        "    \"batch_size\": 64,\n",
        "    \"n_dense_blocks\": 3,\n",
        "    \"n_conv_each_block\": [6,16,14],\n",
        "    \"growth_rate\": 24,\n",
        "    \"dropout\": 0.2,\n",
        "    \"base_learning_rate\": 0.001,\n",
        "    \"warmup_iter\": 7500,\n",
        "    \"main_sched_epochs_kick_in\": 70,\n",
        "    \"n_epochs\": 180,\n",
        "    \"weight_decay\": 0.007,\n",
        "    \"transform_stack_name\": \"default\"\n",
        "},\n",
        "{\n",
        "    \"experiment_name\": \"near_zero_weight_decay\",\n",
        "    \"batch_size\": 64,\n",
        "    \"n_dense_blocks\": 3,\n",
        "    \"n_conv_each_block\": [6,16,14],\n",
        "    \"growth_rate\": 24,\n",
        "    \"dropout\": 0.2,\n",
        "    \"base_learning_rate\": 0.001,\n",
        "    \"warmup_iter\": 7500,\n",
        "    \"main_sched_epochs_kick_in\": 70,\n",
        "    \"n_epochs\": 180,\n",
        "    \"weight_decay\": 0.00005,\n",
        "    \"transform_stack_name\": \"default\"\n",
        "},\n",
        "{\n",
        "    \"experiment_name\": \"large_warmup\",\n",
        "    \"batch_size\": 64,\n",
        "    \"n_dense_blocks\": 3,\n",
        "    \"n_conv_each_block\": [6,16,14],\n",
        "    \"growth_rate\": 24,\n",
        "    \"dropout\": 0.2,\n",
        "    \"base_learning_rate\": 0.0005,\n",
        "    \"warmup_iter\": 20000,\n",
        "    \"main_sched_epochs_kick_in\": 90,\n",
        "    \"n_epochs\": 180,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"transform_stack_name\": \"default\"\n",
        "},\n",
        "{\n",
        "    \"experiment_name\": \"batch32_lr_double\",\n",
        "    \"batch_size\": 32,\n",
        "    \"n_dense_blocks\": 3,\n",
        "    \"n_conv_each_block\": [6,16,14],\n",
        "    \"growth_rate\": 24,\n",
        "    \"dropout\": 0.2,\n",
        "    \"base_learning_rate\": 0.002,\n",
        "    \"warmup_iter\": 7500,\n",
        "    \"main_sched_epochs_kick_in\": 70,\n",
        "    \"n_epochs\": 180,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"transform_stack_name\": \"default\"\n",
        "},\n",
        "{\n",
        "    \"experiment_name\": \"big_128batch\",\n",
        "    \"batch_size\": 128,\n",
        "    \"n_dense_blocks\": 3,\n",
        "    \"n_conv_each_block\": [6,16,14],\n",
        "    \"growth_rate\": 24,\n",
        "    \"dropout\": 0.2,\n",
        "    \"base_learning_rate\": 0.001,\n",
        "    \"warmup_iter\": 7500,\n",
        "    \"main_sched_epochs_kick_in\": 70,\n",
        "    \"n_epochs\": 180,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"transform_stack_name\": \"default\"\n",
        "},\n",
        "{\n",
        "    \"experiment_name\": \"light_data_aug\",\n",
        "    \"batch_size\": 64,\n",
        "    \"n_dense_blocks\": 3,\n",
        "    \"n_conv_each_block\": [6,16,14],\n",
        "    \"growth_rate\": 24,\n",
        "    \"dropout\": 0.2,\n",
        "    \"base_learning_rate\": 0.001,\n",
        "    \"warmup_iter\": 7500,\n",
        "    \"main_sched_epochs_kick_in\": 70,\n",
        "    \"n_epochs\": 180,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"transform_stack_name\": \"same_as_default_but_lighter\"\n",
        "}\n",
        "]\n",
        "parameters_base = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c3e09b11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3e09b11",
        "outputId": "0169a2b2-0e89-4dbb-c049-d20c299fed61"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tot size 2885419008 bytes, for 3669 images\n",
            "tot size 1442316288 bytes, for 1834 images\n",
            "tot size 1451753472 bytes, for 1846 images\n",
            "Done in  258.4708516597748  sec\n",
            "Best accuracy  17.230098146128682\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tot size 2885419008 bytes, for 3669 images\n",
            "tot size 1442316288 bytes, for 1834 images\n",
            "tot size 1451753472 bytes, for 1846 images\n",
            "Done in  26.26095986366272  sec\n",
            "Best accuracy  6.9792802617230105\n"
          ]
        }
      ],
      "source": [
        "total_results = []\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "### Actual run of the experiments\n",
        "for par in parameters_base:\n",
        "    total_results.append(run_experiment(par))\n",
        "    # update partial results after each experiment\n",
        "    df_combined = pd.concat(total_results, ignore_index=True)\n",
        "    df_combined.to_csv(f\"./results_{timestamp}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "878a6c26",
      "metadata": {
        "id": "878a6c26"
      },
      "source": [
        "## Part 2: fine-tune an existing network\n",
        "\n",
        "Your goal is to fine-tune a pretrained ResNet-18 model on `OxfordPetDataset`. Use the implementation provided by PyTorch, i.e. the opposite of part 1. Specifically, use the PyTorch ResNet-18 model pretrained on ImageNet-1K (V1). Divide your fine-tuning into two parts:\n",
        "\n",
        "### 2A.\n",
        " First, fine-tune the ResNet-18 with the same training hyperparameters you used for your best model in part 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcca9a89",
      "metadata": {
        "id": "dcca9a89"
      },
      "source": [
        "### 2B.\n",
        "Then, tweak the training hyperparameters in order to increase the accuracy on the test split. Justify your choices by analyzing the training plots and/or citing sources that guided you in your decisions â€” papers, blog posts, YouTube videos, or whatever else you may find useful. You should consider yourselves satisfied once you obtain a classification accuracy on the test split of ~90%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96c1b425",
      "metadata": {
        "id": "96c1b425"
      },
      "outputs": [],
      "source": [
        "parameters_finetune_base = [\n",
        "{\n",
        "    \"experiment_name\": \"delete_it\",\n",
        "    \"batch_size\": 64,\n",
        "    \"dropout\": 0.2,\n",
        "    \"base_learning_rate\": 0.001,\n",
        "    \"warmup_iter\": 1500,\n",
        "    \"main_sched_epochs_kick_in\": 30,\n",
        "    \"n_epochs\": 2,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"transform_stack_name\": \"default\"\n",
        "},\n",
        "{\n",
        "    \"experiment_name\": \"same_as_densenet\",\n",
        "    \"batch_size\": 64,\n",
        "    \"dropout\": 0.2,\n",
        "    \"base_learning_rate\": 0.001,\n",
        "    \"warmup_iter\": 1500,\n",
        "    \"main_sched_epochs_kick_in\": 30,\n",
        "    \"n_epochs\": 70,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"transform_stack_name\": \"default\"\n",
        "},\n",
        "{\n",
        "    \"experiment_name\": \"better_model\",\n",
        "    \"batch_size\": 64,\n",
        "    \"dropout\": 0.2,\n",
        "    \"base_learning_rate\": 0.0001,\n",
        "    \"warmup_iter\": 1500,\n",
        "    \"main_sched_epochs_kick_in\": 30,\n",
        "    \"n_epochs\": 70,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"transform_stack_name\": \"trivial_augument_wide\"\n",
        "},\n",
        "{\n",
        "    \"experiment_name\": \"better_model_smaller_batches\",\n",
        "    \"batch_size\": 32,\n",
        "    \"dropout\": 0.2,\n",
        "    \"base_learning_rate\": 0.0001,\n",
        "    \"warmup_iter\": 1500,\n",
        "    \"main_sched_epochs_kick_in\": 30,\n",
        "    \"n_epochs\": 70,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"transform_stack_name\": \"trivial_augument_wide\"\n",
        "},\n",
        "{\n",
        "    \"experiment_name\": \"better_model_larger_batches\",\n",
        "    \"batch_size\": 128,\n",
        "    \"dropout\": 0.2,\n",
        "    \"base_learning_rate\": 0.0001,\n",
        "    \"warmup_iter\": 1500,\n",
        "    \"main_sched_epochs_kick_in\": 30,\n",
        "    \"n_epochs\": 70,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"transform_stack_name\": \"trivial_augument_wide\"\n",
        "},\n",
        "{\n",
        "    \"experiment_name\": \"better_model_high_dropout\",\n",
        "    \"batch_size\": 64,\n",
        "    \"dropout\": 0.4,\n",
        "    \"base_learning_rate\": 0.0001,\n",
        "    \"warmup_iter\": 1500,\n",
        "    \"main_sched_epochs_kick_in\": 30,\n",
        "    \"n_epochs\": 70,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"transform_stack_name\": \"trivial_augument_wide\"\n",
        "},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eb4bdad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eb4bdad",
        "outputId": "1085f9b0-bcf0-416a-ad9d-f081e4f66009"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "total_results = []\n",
        "\n",
        "### Actual run of the experiments\n",
        "for par in parameters_finetune_base:\n",
        "    total_results.append(run_experiment(par))\n",
        "    # update partial results after each experiment\n",
        "    df_combined = pd.concat(total_results, ignore_index=True)\n",
        "    df_combined.to_csv(f\"./results_finetune_{timestamp}.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
