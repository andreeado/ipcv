{"cells":[{"cell_type":"markdown","metadata":{"id":"MNBgGYg_lpVN"},"source":["# **Product Recognition of Books**\n","\n","## Image Processing and Computer Vision - Assignment Module \\#1\n","\n","\n","Contacts:\n","\n","- Prof. Giuseppe Lisanti -> giuseppe.lisanti@unibo.it\n","- Prof. Samuele Salti -> samuele.salti@unibo.it\n","- Alex Costanzino -> alex.costanzino@unibo.it\n","- Francesco Ballerini -> francesco.ballerini4@unibo.it"]},{"cell_type":"markdown","metadata":{"id":"R552o2Anyj8T"},"source":["Computer vision-based object detection techniques can be applied in library or bookstore settings to build a system that identifies books on shelves.\n","\n","Such a system could assist in:\n","* Helping visually impaired users locate books by title/author;\n","* Automating inventory management (e.g., detecting misplaced or out-of-stock books);\n","* Enabling faster book retrieval by recognizing spine text or cover designs."]},{"cell_type":"markdown","metadata":{"id":"DW42NlZsyTv0"},"source":["## Task\n","Develop a computer vision system that, given a reference image for each book, is able to identify such book from one picture of a shelf.\n","\n","<figure>\n","<a href=\"https://ibb.co/pvLVjbM5\"><img src=\"https://i.ibb.co/svVx9bNz/example.png\" alt=\"example\" border=\"0\"></a>\n","</figure>\n","\n","For each type of product displayed on the shelf, the system should compute a bounding box aligned with the book spine or cover and report:\n","1. Number of instances;\n","1. Dimension of each instance (area in pixel of the bounding box that encloses each one of them);\n","1. Position in the image reference system of each instance (four corners of the bounding box that enclose them);\n","1. Overlay of the bounding boxes on the scene images.\n","\n","<font color=\"red\"><b>Each step of this assignment must be solved using traditional computer vision techniques.</b></font>\n","\n","#### Example of expected output\n","```\n","Book 0 - 2 instance(s) found:\n","  Instance 1 {top_left: (100,200), top_right: (110, 220), bottom_left: (10, 202), bottom_right: (10, 208), area: 230px}\n","  Instance 2 {top_left: (90,310), top_right: (95, 340), bottom_left: (24, 205), bottom_right: (23, 234), area: 205px}\n","Book 1 – 1 instance(s) found:\n",".\n",".\n",".\n","```"]},{"cell_type":"markdown","metadata":{"id":"9fIbZJKq16ba"},"source":["## Data\n","Two folders of images are provided:\n","* **Models**: contains one reference image for each product that the system should be able to identify;\n","* **Scenes**: contains different shelve pictures to test the developed algorithm in different scenarios."]},{"cell_type":"markdown","metadata":{"id":"5KRBeGbKsEDe"},"source":["## Evaluation criteria\n","1. **Clarity and conciseness**. Present your work in a readable way: format your code and comment every important step;\n","\n","2. **Procedural correctness**. There are several ways to solve the assignment. Design your own sound approach and justify every decision you make;\n","\n","3. **Correctness of results**. Try to solve as many instances as possible. You should be able to solve all the instances of the assignment, however, a thoroughly justified and sound procedure with a lower number of solved instances will be valued **more** than a poorly designed and justified approach that solves more or all instances."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!cp -r /content/drive/MyDrive/Colab_Notebooks/IPCV/dataset.zip ./\n","!unzip dataset.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V7mwkB15n26u","executionInfo":{"status":"ok","timestamp":1759772667555,"user_tz":-120,"elapsed":21366,"user":{"displayName":"Ale Pica","userId":"11047508326906316665"}},"outputId":"48243b45-6c38-45bd-e325-d78d39572d4f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Archive:  dataset.zip\n","   creating: dataset/\n","   creating: dataset/scenes/\n","  inflating: dataset/.DS_Store       \n","  inflating: __MACOSX/dataset/._.DS_Store  \n","   creating: dataset/models/\n","  inflating: dataset/scenes/scene_9.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_9.jpg  \n","  inflating: dataset/scenes/scene_8.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_8.jpg  \n","  inflating: dataset/scenes/scene_20.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_20.jpg  \n","  inflating: dataset/scenes/scene_21.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_21.jpg  \n","  inflating: dataset/scenes/scene_23.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_23.jpg  \n","  inflating: dataset/scenes/scene_22.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_22.jpg  \n","  inflating: dataset/scenes/scene_26.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_26.jpg  \n","  inflating: dataset/scenes/scene_27.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_27.jpg  \n","  inflating: dataset/scenes/scene_25.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_25.jpg  \n","  inflating: dataset/scenes/scene_19.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_19.jpg  \n","  inflating: dataset/scenes/scene_18.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_18.jpg  \n","  inflating: dataset/scenes/scene_24.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_24.jpg  \n","  inflating: dataset/scenes/scene_15.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_15.jpg  \n","  inflating: dataset/scenes/scene_14.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_14.jpg  \n","  inflating: dataset/scenes/scene_28.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_28.jpg  \n","  inflating: dataset/scenes/scene_16.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_16.jpg  \n","  inflating: dataset/scenes/scene_17.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_17.jpg  \n","  inflating: dataset/scenes/scene_13.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_13.jpg  \n","  inflating: dataset/scenes/scene_12.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_12.jpg  \n","  inflating: dataset/scenes/scene_10.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_10.jpg  \n","  inflating: dataset/scenes/scene_11.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_11.jpg  \n","  inflating: dataset/scenes/scene_5.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_5.jpg  \n","  inflating: dataset/scenes/scene_4.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_4.jpg  \n","  inflating: dataset/scenes/scene_6.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_6.jpg  \n","  inflating: dataset/scenes/scene_7.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_7.jpg  \n","  inflating: dataset/scenes/scene_3.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_3.jpg  \n","  inflating: dataset/scenes/scene_2.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_2.jpg  \n","  inflating: dataset/scenes/scene_0.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_0.jpg  \n","  inflating: dataset/scenes/scene_1.jpg  \n","  inflating: __MACOSX/dataset/scenes/._scene_1.jpg  \n","  inflating: dataset/models/model_9.png  \n","  inflating: __MACOSX/dataset/models/._model_9.png  \n","  inflating: dataset/models/model_8.png  \n","  inflating: __MACOSX/dataset/models/._model_8.png  \n","  inflating: dataset/models/.DS_Store  \n","  inflating: __MACOSX/dataset/models/._.DS_Store  \n","  inflating: dataset/models/model_17.png  \n","  inflating: __MACOSX/dataset/models/._model_17.png  \n","  inflating: dataset/models/model_16.png  \n","  inflating: __MACOSX/dataset/models/._model_16.png  \n","  inflating: dataset/models/model_14.png  \n","  inflating: __MACOSX/dataset/models/._model_14.png  \n","  inflating: dataset/models/model_15.png  \n","  inflating: __MACOSX/dataset/models/._model_15.png  \n","  inflating: dataset/models/model_11.png  \n","  inflating: __MACOSX/dataset/models/._model_11.png  \n","  inflating: dataset/models/model_10.png  \n","  inflating: __MACOSX/dataset/models/._model_10.png  \n","  inflating: dataset/models/model_12.png  \n","  inflating: __MACOSX/dataset/models/._model_12.png  \n","  inflating: dataset/models/model_13.png  \n","  inflating: __MACOSX/dataset/models/._model_13.png  \n","  inflating: dataset/models/model_21.png  \n","  inflating: __MACOSX/dataset/models/._model_21.png  \n","  inflating: dataset/models/model_20.png  \n","  inflating: __MACOSX/dataset/models/._model_20.png  \n","  inflating: dataset/models/model_18.png  \n","  inflating: __MACOSX/dataset/models/._model_18.png  \n","  inflating: dataset/models/model_19.png  \n","  inflating: __MACOSX/dataset/models/._model_19.png  \n","  inflating: dataset/models/model_3.png  \n","  inflating: __MACOSX/dataset/models/._model_3.png  \n","  inflating: dataset/models/model_2.png  \n","  inflating: __MACOSX/dataset/models/._model_2.png  \n","  inflating: dataset/models/model_0.png  \n","  inflating: __MACOSX/dataset/models/._model_0.png  \n","  inflating: dataset/models/model_1.png  \n","  inflating: __MACOSX/dataset/models/._model_1.png  \n","  inflating: dataset/models/model_5.png  \n","  inflating: __MACOSX/dataset/models/._model_5.png  \n","  inflating: dataset/models/model_4.png  \n","  inflating: __MACOSX/dataset/models/._model_4.png  \n","  inflating: dataset/models/model_6.png  \n","  inflating: __MACOSX/dataset/models/._model_6.png  \n","  inflating: dataset/models/model_7.png  \n","  inflating: __MACOSX/dataset/models/._model_7.png  \n"]}]},{"cell_type":"code","source":["# Demo: run improved multi-instance detector for one model/scene\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"ElbjPd5AnAMu","executionInfo":{"status":"ok","timestamp":1759772568236,"user_tz":-120,"elapsed":44,"user":{"displayName":"Ale Pica","userId":"11047508326906316665"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XNAu14NrEHuc"},"outputs":[],"source":["# Prepare paths\n","scenes_paths = [f\"dataset/scenes/scene_{i}.jpg\" for i in range(29)]\n","model_paths = [f\"dataset/models/model_{i}.png\" for i in range(22)]\n","\n","# Load images (scenes color, scenes grayscale, models grayscale)\n","scenes = [cv2.imread(path) for path in scenes_paths]\n","scenes_grayscale = [cv2.cvtColor(scene, cv2.COLOR_BGR2GRAY) for scene in scenes]\n","models = [cv2.imread(path, cv2.IMREAD_GRAYSCALE) for path in model_paths]\n","\n","# Precompute SIFT keypoints and descriptors for all models and scenes for performance\n","sift = cv2.SIFT_create()\n","models_kp_des = {}  # dict: idx -> {img, kp, des}\n","for i, img in enumerate(models):\n","    if img is None:\n","        models_kp_des[i] = {'img': None, 'kp': None, 'des': None}\n","        continue\n","    kp, des = sift.detectAndCompute(img, None)\n","    models_kp_des[i] = {'img': img, 'kp': kp, 'des': des}\n","\n","scenes_kp_des = {}  # dict: idx -> {img_color, img_gray, kp, des}\n","for i, (img_color, img_gray) in enumerate(zip(scenes, scenes_grayscale)):\n","    if img_gray is None or img_color is None:\n","        scenes_kp_des[i] = {'img_color': img_color, 'img_gray': img_gray, 'kp': None, 'des': None}\n","        continue\n","    kp_s, des_s = sift.detectAndCompute(img_gray, None)\n","    scenes_kp_des[i] = {'img_color': img_color, 'img_gray': img_gray, 'kp': kp_s, 'des': des_s}\n","\n","print('Precomputed SIFT keypoints/descriptors for', len(models_kp_des), 'models and', len(scenes_kp_des), 'scenes')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":222,"status":"aborted","timestamp":1759772568370,"user":{"displayName":"Ale Pica","userId":"11047508326906316665"},"user_tz":-120},"id":"Q9R-fGrnMw21"},"outputs":[],"source":["# Improved multi-instance detection utilities (adapted from BL implementation)\n","from dataclasses import dataclass\n","\n","def compute_rectangularity(bounds):\n","    x, y, w, h = cv2.boundingRect(bounds)\n","    bounds_area = cv2.contourArea(bounds)\n","    rect_area = w * h if w * h > 0 else 1\n","    return bounds_area / rect_area\n","\n","@dataclass\n","class ProcessedModel:\n","  image: np.array\n","  keypoints: np.array\n","  descriptors: np.array\n","\n","lowes_ratio = 0.72\n","min_match_percent = 0.01\n","ransac_error = 3\n","\n","sift_scene = cv2.SIFT_create(sigma=0.4)\n","\n","def instance_in_scene(processed_model: ProcessedModel, img_scene: np.array, debug=False):\n","  \"\"\"Given a processed model (image, kp, des) and a grayscale scene image,\n","  this computes one instance (if any), masks it out in the provided scene image and returns the polygon (4 corners) in scene coords.\"\"\"\n","  # SIFT feature extraction on the scene\n","  kp_scene, des_scene = sift_scene.detectAndCompute(img_scene, None)\n","\n","  # Feature matching\n","  matcher = cv2.BFMatcher()\n","  if processed_model.descriptors is None or des_scene is None:\n","    return None\n","  matches = matcher.knnMatch(processed_model.descriptors, des_scene, k=2)\n","\n","  # Apply Lowe's ratio test\n","  good_matches = []\n","  for m_n in matches:\n","      if len(m_n) != 2:\n","          continue\n","      m, n = m_n\n","      if m.distance < lowes_ratio * n.distance:\n","          good_matches.append(m)\n","\n","  if len(good_matches) < min_match_percent * len(matches):\n","    return None\n","\n","  h, w = processed_model.image.shape\n","  pts = np.float32([[0,0], [0,h-1], [w-1,h-1], [w-1,0]])\n","\n","  # Extract source and destination points\n","  src_pts = np.float32([processed_model.keypoints[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n","  dst_pts = np.float32([kp_scene[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n","\n","  try:\n","    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, ransac_error)\n","    if M is None or mask is None:\n","        return None\n","  except:\n","    return None\n","\n","  # Transform model corners to scene coordinates\n","  dst = cv2.perspectiveTransform(np.array([pts]), M)\n","\n","  # Check validity\n","  area = int(cv2.contourArea(dst))\n","  if not cv2.isContourConvex(dst) or area < 20 or compute_rectangularity(dst) < 0.6:\n","      return None\n","\n","  # Mask scene in-place so repeated calls won't return the same instance\n","  img_scene[:] = cv2.fillPoly(img_scene.copy(), [np.int32(dst)], (0, 0, 0))\n","\n","  if debug:\n","    img_debug = cv2.drawMatches(processed_model.image, processed_model.keypoints, img_scene, kp_scene, good_matches, None, matchesMask=mask.ravel().tolist(), flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n","    plt.imshow(img_debug)\n","    plt.show()\n","\n","  return dst[0,:,:]\n","\n","# High-level runner: for a given model and scene image, iteratively find instances by masking each found instance and rerunning\n","def detect_multiple_instances(processed_model: ProcessedModel, img_scene_color: np.array, img_scene_gray: np.array, max_instances=10, debug=False):\n","    results = []\n","    # work on a copy of the grayscale scene that we will modify (masking out found instances)\n","    working_gray = img_scene_gray.copy()\n","    for i in range(max_instances):\n","        inst = instance_in_scene(processed_model, working_gray, debug=debug)\n","        if inst is None:\n","            break\n","        # compute axis-aligned bbox and area for reporting\n","        pts = np.int32(inst)\n","        x_coords = pts[:,0]\n","        y_coords = pts[:,1]\n","        x_min, x_max = int(x_coords.min()), int(x_coords.max())\n","        y_min, y_max = int(y_coords.min()), int(y_coords.max())\n","        area = int((x_max - x_min) * (y_max - y_min))\n","        results.append({'corners': pts.tolist(), 'bbox': (x_min, y_min, x_max, y_max), 'area': area})\n","    return results\n","\n","# Helper to run across all models/scenes using precomputed kp/des\n","def run_all_models_on_scenes(models_kp_des, scenes_kp_des, min_matches=30, max_instances=10, visualize=True):\n","    all_results = {}\n","    for i_model, mdl in models_kp_des.items():\n","        if mdl.get('img') is None:\n","            continue\n","        processed = ProcessedModel(mdl['img'], mdl['kp'], mdl['des'])\n","        for i_scene, sc in scenes_kp_des.items():\n","            if sc.get('img_gray') is None:\n","                continue\n","            instances = detect_multiple_instances(processed, sc['img_color'], sc['img_gray'], max_instances=max_instances)\n","            all_results[(i_model, i_scene)] = instances\n","            print(f'Model {i_model} in Scene {i_scene}: found {len(instances)} instance(s)')\n","            if visualize and len(instances) > 0:\n","                img_draw = sc['img_color'].copy()\n","                for inst in instances:\n","                    x_min, y_min, x_max, y_max = inst['bbox']\n","                    cv2.rectangle(img_draw, (x_min, y_min), (x_max, y_max), (0,255,0), 3)\n","                img_rgb = cv2.cvtColor(img_draw, cv2.COLOR_BGR2RGB)\n","                plt.figure(figsize=(10,6))\n","                plt.imshow(img_rgb)\n","                plt.title(f'Model {i_model} — Scene {i_scene} — {len(instances)} instances')\n","                plt.axis('off')\n","                plt.show()\n","    return all_results\n"]},{"cell_type":"markdown","metadata":{"id":"54sB5O62FuzB"},"source":["ccc"]},{"cell_type":"code","source":["# Choose indices to test (change these to try other pairs)\n","model_idx = 0\n","scene_idx = 26\n","max_instances = 10\n","# Prepare processed model\n","try:\n","    mdl = models_kp_des[model_idx]\n","    processed = ProcessedModel(mdl['img'], mdl['kp'], mdl['des'])\n","    sc = scenes_kp_des[scene_idx]\n","    img_scene_color = sc['img_color']\n","    img_scene_gray = sc['img_gray']\n","    print('Using precomputed data from notebook variables')\n","except Exception as e:\n","    print('Falling back to loading from disk:', e)\n","    model_paths = [f\"dataset/models/model_{i}.png\" for i in range(22)]\n","    scene_paths = [f\"dataset/scenes/scene_{i}.jpg\" for i in range(29)]\n","    img_model = cv2.imread(model_paths[model_idx], cv2.IMREAD_GRAYSCALE)\n","    img_scene_color = cv2.imread(scene_paths[scene_idx])\n","    img_scene_gray = cv2.cvtColor(img_scene_color, cv2.COLOR_BGR2GRAY)\n","    sift = cv2.SIFT_create()\n","    kp_m, des_m = sift.detectAndCompute(img_model, None)\n","    processed = ProcessedModel(img_model, kp_m, des_m)\n","\n","# Run multi-instance detection\n","instances = detect_multiple_instances(processed, img_scene_color, img_scene_gray, max_instances=max_instances)\n","print(f'Model {model_idx} in Scene {scene_idx}: found {len(instances)} instance(s)')\n","\n","# Print and visualize results\n","img_draw = img_scene_color.copy()\n","for k, inst in enumerate(instances, start=1):\n","    x_min, y_min, x_max, y_max = inst['bbox']\n","    area = inst['area']\n","    corners = inst['corners']\n","    print(f'  Instance {k} -> top_left=({x_min},{y_min}), top_right=({x_max},{y_min}), bottom_left=({x_min},{y_max}), bottom_right=({x_max},{y_max}), area={area}px')\n","    cv2.rectangle(img_draw, (x_min, y_min), (x_max, y_max), (0, 255, 0), 3)\n","\n","plt.figure(figsize=(12,8))\n","plt.imshow(cv2.cvtColor(img_draw, cv2.COLOR_BGR2RGB))\n","plt.title(f'Model {model_idx} — Scene {scene_idx} — {len(instances)} instances')\n","plt.axis('off')\n","plt.show()\n","\n","# Optional: show the model and the grayscale scene for reference\n","plt.figure(figsize=(12,5))\n","plt.subplot(1,2,1)\n","plt.imshow(processed.image, cmap='gray')\n","plt.title(f'Model {model_idx}')\n","plt.axis('off')\n","plt.subplot(1,2,2)\n","plt.imshow(img_scene_gray, cmap='gray')\n","plt.title(f'Scene {scene_idx} (grayscale) — masked after detections')\n","plt.axis('off')\n","plt.show()"],"metadata":{"id":"fBoeIaonnc5W","executionInfo":{"status":"aborted","timestamp":1759772568378,"user_tz":-120,"elapsed":229,"user":{"displayName":"Ale Pica","userId":"11047508326906316665"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}